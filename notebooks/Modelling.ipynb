{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groundwater level prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import attrgetter\n",
    "\n",
    "# allow plots to appear within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Groundwater level sensors (Ljubljana polje aquifer) with corresponding weather data for Ljubljana from 2010-01-01 to 2017-12-31:\n",
    "- `85065` // Lj. - Flajšmanova (Fip-1/04)\n",
    "- `85012` // Roje (V-01)\n",
    "- `85064` // Lj-Bratislavska (Brp-1a/04)\n",
    "- `85030` // Kleče (0541)\n",
    "\n",
    "Data columns:\n",
    "- `date`: measurement date\n",
    "- `level`: groundwater level `[m]`\n",
    "- `sun_duration`: sun duration `[h]`\n",
    "- `cloud_cover`: cloud cover `[%]`\n",
    "- `precipitation`: precipitation `[mm]`\n",
    "- `snow_accumulation`: daily snow accumulation `[cm]`\n",
    "- `snow_depth`: snow blanket depth `[cm]`\n",
    "- `temperature_avg`: average temperature `[°C]`\n",
    "- `temperature_min`: minimum temperature `[°C]`\n",
    "- `temperature_max`: maximum temperature `[°C]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "sensor_id = 85065\n",
    "df = pd.read_csv(f'../data/ground/{sensor_id}.csv', index_col='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Water level change\n",
    "\n",
    "We are modelling water level change, not absolute water level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df['level'] - df['level'].shift(1)\n",
    "df.insert(1, 'level_diff', diff)\n",
    "#odsrani prvo vrstico ker nima razlike\n",
    "df = df[1:]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generira dneve ker 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20... ker bolj ko gremo nazaj manj gosto nas zanimajo dnevi\n",
    "def get_range(min, max):\n",
    "    r = range(min, max)\n",
    "    r = []\n",
    "    i = min\n",
    "    while i <= max:\n",
    "        r.append(i)\n",
    "        #navzdol zaokroži logaritem\n",
    "        e = math.floor(math.log10(i))\n",
    "        d = 10 ** e\n",
    "        if i < 10 ** (e + 1) / 2:\n",
    "            d = math.ceil(d / 2)\n",
    "        i += d\n",
    "    return r\n",
    "\n",
    "\n",
    "def shift_features(dataset, blacklist, max_shift=20, horizon=3):\n",
    "    days = get_range(1, max_shift)\n",
    "    #print(days)\n",
    "    for feature_name in list(dataset.columns):\n",
    "        #print(feature_name)\n",
    "        for i in days:\n",
    "            #generira stolpec ki pove kakšen je bil parameter i dni nazaj (če feature ni v blacklistu)\n",
    "            #če je vremenska napoved možna za 3 dni ne bomo mogli za 3 dni naprej dobiti podatkov o levelu podtalnice\n",
    "            #izpred 1 ali dva dni, lahko pa bomo dobili podatek izpred treh dni    \n",
    "            if feature_name in blacklist and i < horizon:\n",
    "                continue\n",
    "            #dodamo stolpec nekega feature name, ki je zamaknjen za i dni\n",
    "            dataset[f'{feature_name}_shift_{str(i)}d'] = dataset[feature_name].shift(i)\n",
    "\n",
    "\n",
    "def average_features(dataset, blacklist, max_average=20):\n",
    "    days = get_range(2, max_average)\n",
    "    #print(days)\n",
    "    for feature_name in list(dataset.columns):\n",
    "        for i in days:\n",
    "            if feature_name in blacklist:\n",
    "                continue\n",
    "            #za nek feuture name gremo z oknom velikosti i (za i zadnjih dni) čez podatke, in po njih izračunamo povprečje\n",
    "            dataset[f'{feature_name}_average_{str(i)}d'] = dataset[feature_name].rolling(i).sum() / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_horizon = 3\n",
    "max_shift = max_average = 20\n",
    "\n",
    "shift_features(df, ['level', 'level_diff'], max_shift, prediction_horizon)\n",
    "average_features(df, ['level', 'level_diff'], max_average)\n",
    "\n",
    "# Drop all rows containing NaNs generated during feature construction.\n",
    "min_row = max_shift + max_average - 1\n",
    "#iloc omogoča iskanje kot v tabeli\n",
    "df = df.iloc[min_row:, :]\n",
    "display(df)\n",
    "\n",
    "'''#dodamo ciljne spremenljivke za 5 prediction horiznov\n",
    "df.insert(1, 'h1', df['level_diff'].shift(-1))\n",
    "df.insert(1, 'h2', df['level_diff'].shift(-2))\n",
    "df.insert(1, 'h3', df['level_diff'].shift(-3))\n",
    "df.insert(1, 'h5', df['level_diff'].shift(-5))\n",
    "df=df.iloc[:-5, :]\n",
    "\n",
    "display(df)'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preselected features\n",
    "selected_features = [\n",
    "    'level_diff_shift_3d',\n",
    "    'level_diff_shift_4d',\n",
    "    'level_diff_shift_5d',\n",
    "    'precipitation_average_4d',\n",
    "    'precipitation_average_3d',\n",
    "    'precipitation_average_5d',\n",
    "    'precipitation_average_6d',\n",
    "    'precipitation_average_7d',\n",
    "    'precipitation_shift_1d_average_2d',\n",
    "    'precipitation_shift_1d_average_3d',\n",
    "    'precipitation_average_2d',\n",
    "    'precipitation_shift_1d_average_4d',\n",
    "    'precipitation_average_8d',\n",
    "    'precipitation_shift_1d_average_5d',\n",
    "    'precipitation_average_9d',\n",
    "    'precipitation_shift_1d',\n",
    "    'precipitation_average_10d',\n",
    "    'precipitation_shift_1d_average_6d',\n",
    "    'precipitation_shift_1d_average_7d',\n",
    "    'precipitation_shift_1d_average_8d'\n",
    "]\n",
    "\n",
    "'''# Additional k-best features\n",
    "k = 15\n",
    "#izberemo vse featurje\n",
    "df_X = df.iloc[:, 6:]\n",
    "#pretvori dataset v tabelo in jo casta v float\n",
    "X = df_X.values.astype(float)\n",
    "y = df['level_diff'].values.astype(float)\n",
    "y_h1 = df['h1'].values.astype(float)\n",
    "y_h2 = df['h2'].values.astype(float)\n",
    "y_h3 = df['h3'].values.astype(float)\n",
    "y_h5 = df['h5'].values.astype(float)\n",
    "\n",
    "#za vsak prediction horizon izberemo ustrezne featurje\n",
    "X_h1=df_X.values.astype(float)\n",
    "selected_features_h1 = [\n",
    "    f'level_shift_{prediction_horizon}d',\n",
    "    f'level_shift_{prediction_horizon + 1}d',\n",
    "    f'level_shift_{prediction_horizon + 2}d',\n",
    "    f'level_diff_shift_{prediction_horizon}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 1}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 2}d',\n",
    "    'sun_duration',\n",
    "    'cloud_cover',\n",
    "    'precipitation',\n",
    "    'snow_accumulation',\n",
    "    'snow_depth',\n",
    "    'temperature_avg',\n",
    "    'temperature_min',\n",
    "    'temperature_max']\n",
    "fs_model_h1 = SelectKBest(f_regression, k=k).fit(X_h1, y_h1)\n",
    "#argsort returns indices that would sort the array\n",
    "#[::-1] reverses the array\n",
    "feature_indices_h1 = fs_model_h1.scores_.argsort()[::-1][0:k]\n",
    "#če featurjev še ni na seznamu jih dodamo\n",
    "for feature in df_X.columns[feature_indices_h1]:\n",
    "    if feature not in selected_features_h1:\n",
    "        selected_features_h1.append(feature)\n",
    "print(1)\n",
    "print(selected_features_h1)\n",
    "\n",
    "X_h2=df_X.values.astype(float)\n",
    "selected_features_h2 = [\n",
    "    f'level_shift_{prediction_horizon}d',\n",
    "    f'level_shift_{prediction_horizon + 1}d',\n",
    "    f'level_shift_{prediction_horizon + 2}d',\n",
    "    f'level_diff_shift_{prediction_horizon}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 1}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 2}d',\n",
    "    'sun_duration',\n",
    "    'cloud_cover',\n",
    "    'precipitation',\n",
    "    'snow_accumulation',\n",
    "    'snow_depth',\n",
    "    'temperature_avg',\n",
    "    'temperature_min',\n",
    "    'temperature_max'\n",
    "]\n",
    "fs_model_h2 = SelectKBest(f_regression, k=k).fit(X_h2, y_h2)\n",
    "feature_indices_h2 = fs_model_h2.scores_.argsort()[::-1][0:k]\n",
    "for feature in df_X.columns[feature_indices_h2]:\n",
    "    if feature not in selected_features_h2:\n",
    "        selected_features_h2.append(feature)\n",
    "print(2)\n",
    "print(selected_features_h2)\n",
    "\n",
    "X_h3=df_X.values.astype(float)\n",
    "selected_features_h3 = [\n",
    "    f'level_shift_{prediction_horizon}d',\n",
    "    f'level_shift_{prediction_horizon + 1}d',\n",
    "    f'level_shift_{prediction_horizon + 2}d',\n",
    "    f'level_diff_shift_{prediction_horizon}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 1}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 2}d',\n",
    "    'sun_duration',\n",
    "    'cloud_cover',\n",
    "    'precipitation',\n",
    "    'snow_accumulation',\n",
    "    'snow_depth',\n",
    "    'temperature_avg',\n",
    "    'temperature_min',\n",
    "    'temperature_max'\n",
    "]\n",
    "fs_model_h3 = SelectKBest(f_regression, k=k).fit(X_h3, y_h3)\n",
    "feature_indices_h3 = fs_model_h3.scores_.argsort()[::-1][0:k]\n",
    "for feature in df_X.columns[feature_indices_h3]:\n",
    "    if feature not in selected_features_h3:\n",
    "        selected_features_h3.append(feature)\n",
    "print(3)\n",
    "print(selected_features_h3)\n",
    "\n",
    "X_h5=df_X.values.astype(float)\n",
    "selected_features_h5 = [\n",
    "    f'level_shift_{prediction_horizon}d',\n",
    "    f'level_shift_{prediction_horizon + 1}d',\n",
    "    f'level_shift_{prediction_horizon + 2}d',\n",
    "    f'level_diff_shift_{prediction_horizon}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 1}d',\n",
    "    f'level_diff_shift_{prediction_horizon + 2}d',\n",
    "    'sun_duration',\n",
    "    'cloud_cover',\n",
    "    'precipitation',\n",
    "    'snow_accumulation',\n",
    "    'snow_depth',\n",
    "    'temperature_avg',\n",
    "    'temperature_min',\n",
    "    'temperature_max'\n",
    "]\n",
    "fs_model_h5 = SelectKBest(f_regression, k=k).fit(X_h5, y_h5)\n",
    "feature_indices_h5 = fs_model_h5.scores_.argsort()[::-1][0:k]\n",
    "for feature in df_X.columns[feature_indices_h5]:\n",
    "    if feature not in selected_features_h5:\n",
    "        selected_features_h5.append(feature)\n",
    "print(5)\n",
    "print(selected_features_h5)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastener feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from typing import Dict, List, Callable, Any, Tuple, Optional, \\\n",
    "    Counter as CounterType, Set\n",
    "from src.random_utils import shuffle\n",
    "from src import random_utils\n",
    "from src.item import Item, EvalItem, Result, Population, flatten_population, FitnessFunction, \\\n",
    "    Genes, EvalItem, RandomFlipMutationStrategy, RandomEveryoneWithEveryone, \\\n",
    "    IntersectionMating, UnionMating, IntersectionMatingWithInformationGain, \\\n",
    "    IntersectionMatingWithWeightedRandomInformationGain, UnevaluatedPopulation, \\\n",
    "    MatingStrategy, MutationStrategy, MatingSelectionStrategy\n",
    "from src import fastener\n",
    "\n",
    "\n",
    "\n",
    "general_model = LinearRegression\n",
    "\n",
    "#sets number of samples and number of semples used for testing\n",
    "n_sample=df.shape[0]\n",
    "n_test=int(n_sample*0.8)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['h1'].values.astype(float))\n",
    "\n",
    "'''labels_train=le.transform(df['h1'].values.astype(float)[:n_test])\n",
    "labels_test=le.transform(df['h1'].values.astype(float)[n_test:])\n",
    "'''\n",
    "labels_train=df['h1'].values.astype(float)[:n_test]\n",
    "labels_test=df['h1'].values.astype(float)[n_test:]\n",
    "\n",
    "XX_train=df.to_numpy()[:n_test, 6:]\n",
    "XX_test=df.to_numpy()[n_test:, 6:]\n",
    "\n",
    "\n",
    "def eval_fun(model: Any, genes: \"Genes\", shuffle_indices: Optional[List[int]] = None) -> \"Result\":\n",
    "    test_data = XX_test[:, genes]\n",
    "    if shuffle_indices:\n",
    "        test_data = test_data.copy()\n",
    "        for j in shuffle_indices:\n",
    "            shuffle(test_data[:, j])\n",
    "    pred = model.predict(test_data)\n",
    "    res = Result(r2_score(labels_test, pred))\n",
    "    return res\n",
    "\n",
    "number_of_genes = XX_train.shape[1]\n",
    "\n",
    "initial_genes = [\n",
    "    [0]\n",
    "]\n",
    "# Select mating strategies\n",
    "mating = RandomEveryoneWithEveryone(pool_size=5, mating_strategy=IntersectionMatingWithWeightedRandomInformationGain(regression=True))\n",
    "\n",
    "# Random mutation\n",
    "mutation = RandomFlipMutationStrategy(1 / number_of_genes)\n",
    "\n",
    "entropy_optimizer = fastener.EntropyOptimizer(\n",
    "    general_model, XX_train, labels_train, eval_fun,\n",
    "    number_of_genes, mating, mutation, initial_genes=initial_genes,\n",
    "    config=fastener.Config(output_folder=\"h1_tree_c\", random_seed=2020, reset_to_pareto_rounds=5)\n",
    ")\n",
    "#entropy_optimizer.config.number_of_rounds=10\n",
    "entropy_optimizer.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.random_utils import shuffle\n",
    "from src import random_utils\n",
    "from src.item import Item, EvalItem, Result, Population, flatten_population, FitnessFunction, \\\n",
    "    Genes, EvalItem, RandomFlipMutationStrategy, RandomEveryoneWithEveryone, \\\n",
    "    IntersectionMating, UnionMating, IntersectionMatingWithInformationGain, \\\n",
    "    IntersectionMatingWithWeightedRandomInformationGain, UnevaluatedPopulation, \\\n",
    "    MatingStrategy, MutationStrategy, MatingSelectionStrategy\n",
    "from src import fastener\n",
    "\n",
    "object = pd.read_pickle(r'log/h1_linear_regression/generation_1000.pickle')\n",
    "values=object.front.values()\n",
    "print(values)\n",
    "m=0\n",
    "for v in values:\n",
    "    if v.result.score>m:\n",
    "        m=v.result.score\n",
    "        win=v\n",
    "feat=df.iloc[:, 6:]\n",
    "selected_features_h1 =feat.iloc[:, win.genes].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = pd.read_pickle(r'log/h1_tree_c/generation_1000.pickle')\n",
    "values=object.front.values()\n",
    "m=0\n",
    "for v in values:\n",
    "    if v.result.score>m:\n",
    "        m=v.result.score\n",
    "        win=v\n",
    "feat=df.iloc[:, 6:]\n",
    "selected_features_h1 =feat.iloc[:, win.genes].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "#df_X = df.iloc[:, 6:]\n",
    "#crete target values for five different horizons\n",
    "#select columns with features\n",
    "X_h1 = df_X[selected_features].values.astype(float)\n",
    "y_h1 = df['level_diff'].values.astype(float)\n",
    "'''X_h2 = df_X[selected_features].values.astype(float)\n",
    "X_h3 = df_X[selected_features].values.astype(float)\n",
    "X_h5 = df_X[selected_features].values.astype(float)'''\n",
    "\n",
    "\n",
    "\n",
    "def r2_calculation_random_forest():\n",
    "    #splits data considering it is a time series\n",
    "    timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    #, max_depth = 10, max_features = 'auto', min_samples_leaf = 4, min_samples_split = 10\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h1, y_h1, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h1:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    '''model=RandomForestRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h2, y_h2, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h2:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h3, y_h3, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h3:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=RandomForestRegressor(bootstrap=False, n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h5, y_h5, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h5:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())'''\n",
    "    \n",
    "def grid_search_random_forest():\n",
    "    #splits data considering it is a time series\n",
    "    timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "    param_search={'max_depth': [10, 50, 70, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],}\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    gsearch = GridSearchCV(estimator=model, cv=timeSeriesCV, param_grid=param_search, verbose=20, scoring='r2', n_jobs=-1)\n",
    "    gsearch.fit(X, y_h1)\n",
    "\n",
    "    #print results\n",
    "    print(gsearch.best_score_)\n",
    "    print(gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_random_forest():    \n",
    "    n_samples=df_X.shape[0]\n",
    "    n_train=int(n_samples*0.9)\n",
    "\n",
    "    #horizon 1\n",
    "    X_h1_train = df_X[selected_features_h1].values.astype(float)[:n_train, :]\n",
    "    X_h1_test = df_X[selected_features_h1].values.astype(float)[n_train:, :]\n",
    "    y_h1_train = df['h1'].values.astype(float)[:n_train]\n",
    "    y_h1_test = df['h1'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_h1_train, y_h1_train)\n",
    "    res_h1=model.predict(X_h1_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h1, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h1_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h1[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 2\n",
    "    X_h2_train = df_X[selected_features_h2].values.astype(float)[:n_train, :]\n",
    "    X_h2_test = df_X[selected_features_h2].values.astype(float)[n_train:, :]\n",
    "    y_h2_train = df['h2'].values.astype(float)[:n_train]\n",
    "    y_h2_test = df['h2'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_h2_train, y_h2_train)\n",
    "    res_h2=model.predict(X_h2_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h2, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h2_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h2[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 3\n",
    "    X_h3_train = df_X[selected_features_h3].values.astype(float)[:n_train, :]\n",
    "    X_h3_test = df_X[selected_features_h3].values.astype(float)[n_train:, :]\n",
    "    y_h3_train = df['h3'].values.astype(float)[:n_train]\n",
    "    y_h3_test = df['h3'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_h3_train, y_h3_train)\n",
    "    res_h3=model.predict(X_h3_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h3, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h3_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h3[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 5\n",
    "    X_h5_train = df_X[selected_features_h5].values.astype(float)[:n_train, :]\n",
    "    X_h5_test = df_X[selected_features_h5].values.astype(float)[n_train:, :]\n",
    "    y_h5_train = df['h5'].values.astype(float)[:n_train]\n",
    "    y_h5_test = df['h5'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_h5_train, y_h5_train)\n",
    "    res_h5=model.predict(X_h5_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h5, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h5_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h5[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: level')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"r2 = r2_score(y_test, y_pred)\n",
    "r2\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "\n",
    "#crete target values for five different horizons\n",
    "#select columns with features\n",
    "X_h1 = df_X[selected_features].values.astype(float)\n",
    "X_h2 = df_X[selected_features].values.astype(float)\n",
    "X_h3 = df_X[selected_features].values.astype(float)\n",
    "X_h5 = df_X[selected_features].values.astype(float)\n",
    "\n",
    "\n",
    "def r2_calculation_linear_regression():\n",
    "    \n",
    "    #splits data considering it is a time series\n",
    "    timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    model=LinearRegression(normalize=True)\n",
    "    cvs=cross_val_score(model, X_h1, y_h1, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h1:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=LinearRegression()\n",
    "    cvs=cross_val_score(model, X_h2, y_h2, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h2:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=LinearRegression()\n",
    "    cvs=cross_val_score(model, X_h3, y_h3, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h3:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=LinearRegression()\n",
    "    cvs=cross_val_score(model, X_h5, y_h5, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h5:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "    \n",
    "def grid_search_linear_regression():\n",
    "    param_search={'max_depth': [10, 50, 70, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],}\n",
    "    model=LinearRegression()\n",
    "    gsearch = GridSearchCV(estimator=model, cv=timeSeriesCV, param_grid=param_search, verbose=20, scoring='r2', n_jobs=7)\n",
    "    gsearch.fit(X, y_h1)\n",
    "\n",
    "    #print results\n",
    "    print(gsearch.best_score_)\n",
    "    print(gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying results(linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_linear_regression():    \n",
    "    n_samples=df_X.shape[0]\n",
    "    n_train=int(n_samples*0.9)\n",
    "\n",
    "    #horizon 1\n",
    "    X_h1_train = df_X[selected_features_h1].values.astype(float)[:n_train, :]\n",
    "    X_h1_test = df_X[selected_features_h1].values.astype(float)[n_train:, :]\n",
    "    y_h1_train = df['h1'].values.astype(float)[:n_train]\n",
    "    y_h1_test = df['h1'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_h1_train, y_h1_train)\n",
    "    res_h1=model.predict(X_h1_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h1, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h1_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h1[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 2\n",
    "    X_h2_train = df_X[selected_features_h2].values.astype(float)[:n_train, :]\n",
    "    X_h2_test = df_X[selected_features_h2].values.astype(float)[n_train:, :]\n",
    "    y_h2_train = df['h2'].values.astype(float)[:n_train]\n",
    "    y_h2_test = df['h2'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_h2_train, y_h2_train)\n",
    "    res_h2=model.predict(X_h2_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h2, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h2_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h2[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 3\n",
    "    X_h3_train = df_X[selected_features_h3].values.astype(float)[:n_train, :]\n",
    "    X_h3_test = df_X[selected_features_h3].values.astype(float)[n_train:, :]\n",
    "    y_h3_train = df['h3'].values.astype(float)[:n_train]\n",
    "    y_h3_test = df['h3'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_h3_train, y_h3_train)\n",
    "    res_h3=model.predict(X_h3_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h3, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h3_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h3[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 5\n",
    "    X_h5_train = df_X[selected_features_h5].values.astype(float)[:n_train, :]\n",
    "    X_h5_test = df_X[selected_features_h5].values.astype(float)[n_train:, :]\n",
    "    y_h5_train = df['h5'].values.astype(float)[:n_train]\n",
    "    y_h5_test = df['h5'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=LinearRegression()\n",
    "    model.fit(X_h5_train, y_h5_train)\n",
    "    res_h5=model.predict(X_h5_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h5, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h5_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h5[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: level')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "\n",
    "#crete target values for five different horizons\n",
    "#select columns with features\n",
    "X_h1 = df_X[selected_features].values.astype(float)\n",
    "X_h2 = df_X[selected_features].values.astype(float)\n",
    "X_h3 = df_X[selected_features].values.astype(float)\n",
    "X_h5 = df_X[selected_features].values.astype(float)\n",
    "\n",
    "\n",
    "def r2_calculation_gradient_boost():\n",
    "    #splits data considering it is a time series\n",
    "    timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h1, y_h1, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h1:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h2, y_h2, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h2:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h3, y_h3, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h3:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h5, y_h5, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h5:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "    \n",
    "def grid_search_gradint_boost():\n",
    "    param_search={'max_depth': [10, 50, 70, 100, None], 'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],}\n",
    "    model=GradientBoostingRegressor(n_estimators=10)\n",
    "    gsearch = GridSearchCV(estimator=model, cv=timeSeriesCV, param_grid=param_search, verbose=20, scoring='r2', n_jobs=7)\n",
    "    gsearch.fit(X, y_h1)\n",
    "\n",
    "    #print results\n",
    "    print(gsearch.best_score_)\n",
    "    print(gsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displaying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results_gradient_boost():    \n",
    "    n_samples=df_X.shape[0]\n",
    "    n_train=int(n_samples*0.9)\n",
    "\n",
    "    #horizon 1\n",
    "    X_h1_train = df_X[selected_features_h1].values.astype(float)[:n_train, :]\n",
    "    X_h1_test = df_X[selected_features_h1].values.astype(float)[n_train:, :]\n",
    "    y_h1_train = df['h1'].values.astype(float)[:n_train]\n",
    "    y_h1_test = df['h1'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    model.fit(X_h1_train, y_h1_train)\n",
    "    res_h1=model.predict(X_h1_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h1, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h1_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h1[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON1: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 2\n",
    "    X_h2_train = df_X[selected_features_h2].values.astype(float)[:n_train, :]\n",
    "    X_h2_test = df_X[selected_features_h2].values.astype(float)[n_train:, :]\n",
    "    y_h2_train = df['h2'].values.astype(float)[:n_train]\n",
    "    y_h2_test = df['h2'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    model.fit(X_h2_train, y_h2_train)\n",
    "    res_h2=model.predict(X_h2_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h2, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h2_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h2[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON2: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 3\n",
    "    X_h3_train = df_X[selected_features_h3].values.astype(float)[:n_train, :]\n",
    "    X_h3_test = df_X[selected_features_h3].values.astype(float)[n_train:, :]\n",
    "    y_h3_train = df['h3'].values.astype(float)[:n_train]\n",
    "    y_h3_test = df['h3'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    model.fit(X_h3_train, y_h3_train)\n",
    "    res_h3=model.predict(X_h3_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h3, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h3_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h3[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON3: level')\n",
    "    plt.show()\n",
    "\n",
    "    #horizon 5\n",
    "    X_h5_train = df_X[selected_features_h5].values.astype(float)[:n_train, :]\n",
    "    X_h5_test = df_X[selected_features_h5].values.astype(float)[n_train:, :]\n",
    "    y_h5_train = df['h5'].values.astype(float)[:n_train]\n",
    "    y_h5_test = df['h5'].values.astype(float)[n_train:]\n",
    "\n",
    "    model=GradientBoostingRegressor(n_estimators=100)\n",
    "    model.fit(X_h5_train, y_h5_train)\n",
    "    res_h5=model.predict(X_h5_test)\n",
    "\n",
    "    predicted=plt.plot(range(n_samples-n_train), res_h5, label='predicted')\n",
    "    real=plt.plot(range(n_samples-n_train), y_h5_test, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: difference')\n",
    "    plt.show()\n",
    "\n",
    "    real_level=df['level'].values.astype(float)[n_train:]\n",
    "    predicted_level=[real_level[0]]\n",
    "    for i in range(1, n_samples-n_train):\n",
    "        predicted_level.append(res_h5[i]+predicted_level[-1])\n",
    "\n",
    "    predicted_level_line=plt.plot(range(n_samples-n_train), predicted_level, label='predicted')\n",
    "    real_level_line=plt.plot(range(n_samples-n_train), real_level, label='real')\n",
    "    plt.legend()\n",
    "    plt.title('HORIZON5: level')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface water level prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "Surface water level sensors with corresponding weather data from 2010-01-01 to 2017-12-31:\n",
    "- `2250` // Otiški Vrh I (Meža)\n",
    "- `2530` // Ruta (Radoljna)\n",
    "- `2620` // Loče (Dravinja)\n",
    "- `3200` // Sveti Janez (Sava Bohinjka)\n",
    "- `3250` // Bodešče (Sava Bohinjka)\n",
    "- `3400` // Mlino I (Jezernica)\n",
    "- `4200` // Suha I (Sora)\n",
    "- `4230` // Zminec (Poljanska Sora)\n",
    "- `4270` // Železniki (Selška Sora)\n",
    "- `4450` // Domžale (Mlinščica-Kanal)\n",
    "- `4515` // Vir (Rača)\n",
    "- `4520` // Podrečje (Rača)\n",
    "- `4570` // Topole (Pšata)\n",
    "- `4575` // Loka (Pšata)\n",
    "- `4650` // Žebnik (Sopota)\n",
    "- `4770` // Sodna vas II (Mestinjščica)\n",
    "- `5040` // Kamin (Ljubljanica)\n",
    "- `5078` // Moste I (Ljubljanica)\n",
    "- `5330` // Borovnica (Borovniščica)\n",
    "- `5425` // Iška vas (Iška)\n",
    "- `5500` // Dvor (Gradaščica)\n",
    "- `6060` // Nazarje (Savinja)\n",
    "- `6068` // Letuš I (Savinja)\n",
    "- `6200` // Laško I (Savinja)\n",
    "- `6220` // Luče (Lučnica)\n",
    "- `6300` // Šoštanj (Paka)\n",
    "- `6340` // Rečica (Paka)\n",
    "- `6350` // Škale (Lepena)\n",
    "- `6385` // Pesje IV (Lepena)\n",
    "- `6400` // Škale (Sopota)\n",
    "- `8454` // Cerkno III (Cerknica)\n",
    "- `8565` // Dolenje (Vipava)\n",
    "\n",
    "Data columns:\n",
    "- date\n",
    "- level,day_time\n",
    "- precipitation\n",
    "- snow_accumulation\n",
    "- temperature_avg\n",
    "- temperature_min\n",
    "- temperature_max\n",
    "- cloud_cover_avg\n",
    "- cloud_cover_min\n",
    "- cloud_cover_max\n",
    "- dew_point_avg\n",
    "- dew_point_min\n",
    "- dew_point_max\n",
    "- humidity_avg\n",
    "- humidity_min\n",
    "- humidity_max\n",
    "- pressure_avg\n",
    "- pressure_min\n",
    "- pressure_max\n",
    "- precipitation_probability_avg\n",
    "- precipitation_probability_min\n",
    "- precipitation_probability_max\n",
    "- precipitation_intensity_avg\n",
    "- precipitation_intensity_min\n",
    "- precipitation_intensity_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_calculation_random_forest()\n",
    "#display_results_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_calculation_gradient_boost()\n",
    "display_results_gradient_boost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_calculation_linear_regression()\n",
    "display_results_linear_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_random_forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(selected_features_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "    \n",
    "    \n",
    "for v in values:\n",
    "    selected_features_h1 =feat.iloc[:, v.genes].columns\n",
    "    X_h1 = df_X[selected_features_h1].values.astype(float)\n",
    "    model=LinearRegression(normalize=True)\n",
    "    #model=RandomForestRegressor(n_estimators=100)\n",
    "    cvs=cross_val_score(model, X_h1, y_h1, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h1:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSeriesCV=TimeSeriesSplit(n_splits=n_folds)\n",
    "object = pd.read_pickle(r'log/h1_tree/generation_1000.pickle')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['h1'].values.astype(float))\n",
    "#y_h1=le.transform(y_h1)\n",
    "print(y_h1)\n",
    "values=object.front.values()\n",
    "for v in values:\n",
    "    selected_features_h1 =feat.iloc[:, v.genes].columns\n",
    "    X_h1 = df_X[selected_features_h1].values.astype(float)\n",
    "    #model=LinearRegression(normalize=True)\n",
    "    model=DecisionTreeClassifier()\n",
    "    cvs=cross_val_score(model, X_h1, y_h1, scoring='r2', cv=timeSeriesCV)\n",
    "    print(\"h1:\")\n",
    "    print(\"R2: \", cvs.mean(), \" stdev: \", cvs.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
